{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyUUvaTSzzYr",
        "outputId": "012c5fce-72ca-4c07-de06-05d49937c118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug  5 19:20:29 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAgVBQy00VSF",
        "outputId": "4f0e1b80-b2a2-4ba1-beb0-458217042d1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting langchain==0.0.248\n",
            "  Obtaining dependency information for langchain==0.0.248 from https://files.pythonhosted.org/packages/80/d6/1160e4df19e54f1a421883873e82d0f6aba2a368c8c9ea46a9d170c585bf/langchain-0.0.248-py3-none-any.whl.metadata\n",
            "  Downloading langchain-0.0.248-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.248) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.248) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.248) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.248) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.248)\n",
            "  Obtaining dependency information for dataclasses-json<0.6.0,>=0.5.7 from https://files.pythonhosted.org/packages/97/5f/e7cc90f36152810cab08b6c9c1125e8bcb9d76f8b3018d101b5f877b386c/dataclasses_json-0.5.14-py3-none-any.whl.metadata\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.11 (from langchain==0.0.248)\n",
            "  Obtaining dependency information for langsmith<0.1.0,>=0.0.11 from https://files.pythonhosted.org/packages/35/5a/0394e665521132bd3a8a0cfa3ffc6569d2627f74c178108a813f4ff27c07/langsmith-0.0.19-py3-none-any.whl.metadata\n",
            "  Downloading langsmith-0.0.19-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.248) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.248) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.248)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.248) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.248) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.248) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.248) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.248) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.248) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.248) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.248) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.248) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.248)\n",
            "  Obtaining dependency information for marshmallow<4.0.0,>=3.18.0 from https://files.pythonhosted.org/packages/ed/3c/cebfdcad015240014ff08b883d1c0c427f2ba45ae8c6572851b6ef136cad/marshmallow-3.20.1-py3-none-any.whl.metadata\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.248) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.248) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.248) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.248) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.248) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.248) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.248) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.248) (1.0.0)\n",
            "Downloading langchain-0.0.248-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Downloading langsmith-0.0.19-py3-none-any.whl (31 kB)\n",
            "Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: marshmallow, openapi-schema-pydantic, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.14 langchain-0.0.248 langsmith-0.0.19 marshmallow-3.20.1 openapi-schema-pydantic-1.2.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting InstructorEmbedding\n",
            "  Obtaining dependency information for InstructorEmbedding from https://files.pythonhosted.org/packages/6c/fc/64375441f43cc9ddc81f76a1a8f516e6d63f5b6ecb67fffdcddc0445f0d3/InstructorEmbedding-1.0.1-py2.py3-none-any.whl.metadata\n",
            "  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl.metadata (20 kB)\n",
            "Collecting pypdf\n",
            "  Obtaining dependency information for pypdf from https://files.pythonhosted.org/packages/fe/e6/a23048e3ae56daf03479f5fe2f49af8d989cab1bcc1f8d2f15de83a3eaea/pypdf-3.14.0-py3-none-any.whl.metadata\n",
            "  Downloading pypdf-3.14.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pinecone-client\n",
            "  Obtaining dependency information for pinecone-client from https://files.pythonhosted.org/packages/98/17/3675b83dca0a032d2750bf04fbfdf78a6e46fa3056eefc2574cdd14661d9/pinecone_client-2.2.2-py3-none-any.whl.metadata\n",
            "  Downloading pinecone_client-2.2.2-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting replicate\n",
            "  Obtaining dependency information for replicate from https://files.pythonhosted.org/packages/5b/73/f94cbd9f8aeab60abf949d816d5da33504fd583e2d53468e708311e8035e/replicate-0.10.0-py3-none-any.whl.metadata\n",
            "  Downloading replicate-0.10.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting openai\n",
            "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/67/78/7588a047e458cb8075a4089d721d7af5e143ff85a2388d4a28c530be0494/openai-0.27.8-py3-none-any.whl.metadata\n",
            "  Downloading openai-0.27.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting shortuuid\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (6.0.1)\n",
            "Collecting loguru>=0.5.0 (from pinecone-client)\n",
            "  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.7.1)\n",
            "Collecting dnspython>=2.0.0 (from pinecone-client)\n",
            "  Obtaining dependency information for dnspython>=2.0.0 from https://files.pythonhosted.org/packages/71/30/deee2ffb94194437c730a1c6230d9310ab5f73926a2549cdab91453616bb/dnspython-2.4.1-py3-none-any.whl.metadata\n",
            "  Downloading dnspython-2.4.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.26.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from replicate) (23.1)\n",
            "Requirement already satisfied: pydantic>1 in /usr/local/lib/python3.10/dist-packages (from replicate) (1.10.12)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n",
            "Downloading pypdf-3.14.0-py3-none-any.whl (269 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.8/269.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_client-2.2.2-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.1/179.1 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading replicate-0.10.0-py3-none-any.whl (22 kB)\n",
            "Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.4.1-py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=f7cfb010056d4cfee1ebf42ee79f17b65a209e879d69289c07e64ae2f97804dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: InstructorEmbedding, shortuuid, pypdf, loguru, dnspython, replicate, pinecone-client, openai, sentence_transformers\n",
            "Successfully installed InstructorEmbedding-1.0.1 dnspython-2.4.1 loguru-0.7.0 openai-0.27.8 pinecone-client-2.2.2 pypdf-3.14.0 replicate-0.10.0 sentence_transformers-2.2.2 shortuuid-1.0.11\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq bitsandbytes==0.40.0 --progress-bar off\n",
        "!pip install -qqq torch==2.0.1 --progress-bar off\n",
        "!pip install -qqq transformers==4.31.0 --progress-bar off\n",
        "!pip install -qqq accelerate==0.21.0 --progress-bar off\n",
        "!pip install -qqq xformers==0.0.20 --progress-bar off\n",
        "!pip install -qqq einops==0.6.1 --progress-bar off\n",
        "!pip install -qqq huggingface-hub==0.16.4 --progress-bar off\n",
        "!pip install -qqq sentencepiece==0.1.99 --progress-bar off\n",
        "!pip install langchain==0.0.248\n",
        "!pip install InstructorEmbedding pypdf sentence_transformers pinecone-client replicate openai shortuuid python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-h-KW7Q0fao",
        "outputId": "a41040f2-a853-4486-d8b6-5c08712f54a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8013'), PosixPath('//172.28.0.1'), PosixPath('http')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-hm-20rpu356f1qlf --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer, pipeline\n",
        "\n",
        "import os\n",
        "import pinecone\n",
        "import openai\n",
        "\n",
        "from langchain.llms import OpenAI, Replicate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.vectorstores import Milvus\n",
        "from langchain.vectorstores import Weaviate\n",
        "from langchain.vectorstores import Qdrant\n",
        "from langchain.text_splitter import (\n",
        "    CharacterTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "OPENAI_API_BASE = os.environ[\"OPENAI_API_BASE\"]\n",
        "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
        "REPLICATE_API_TOKEN = os.environ[\"REPLICATE_API_TOKEN\"]\n",
        "PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]\n",
        "PINECONE_ENV = os.environ[\"PINECONE_ENV\"]\n",
        "INDEX_NAME = os.environ[\"INDEX_NAME\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tZVGMZRESpc"
      },
      "outputs": [],
      "source": [
        "def get_qa_retriever(llm):\n",
        "    text_field = \"text\"\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        model=\"text-embedding-ada-002\",\n",
        "        openai_api_key=OPENAI_API_KEY\n",
        "    )\n",
        "\n",
        "    # embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n",
        "    #                                            model_kwargs={\"device\": \"cuda\"})\n",
        "\n",
        "    pinecone.init(\n",
        "       api_key=PINECONE_API_KEY,\n",
        "       environment=PINECONE_ENV\n",
        "    )\n",
        "\n",
        "    index = pinecone.Index(INDEX_NAME)\n",
        "    vector_store = Pinecone(\n",
        "       index, embeddings.embed_query, text_field\n",
        "    )\n",
        "\n",
        "    template = \"\"\"You are a helpful data science tutor chatbot having a conversation with a human.\n",
        "                Follow exactly these 3 steps:\n",
        "                1. Read the Context below combining this information with the Question\n",
        "                2. Answer the Question using only the information in the Context section\n",
        "                3. Show the source for your answers\n",
        "\n",
        "                Context : {context}\n",
        "                User Question: {question}\n",
        "\n",
        "                The answer and your response should only come from the Context section. If the question is\n",
        "                not related to the Context section, politely respond that you are tuned to only answer questions that\n",
        "                are related to data science. Use as much detail as possible when responding.\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"question\", \"context\"], template=template\n",
        "    )\n",
        "\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type='stuff',\n",
        "        retriever=vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3}),\n",
        "        verbose=True,\n",
        "        chain_type_kwargs={\n",
        "            \"verbose\": True,\n",
        "            \"prompt\": prompt\n",
        "        }\n",
        "        # max_tokens_limit=4096\n",
        "    )\n",
        "\n",
        "    return qa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm(model_name):\n",
        "\n",
        "    # llm = ChatOpenAI(model_name=model_name,\n",
        "    #                  temperature=0)\n",
        "\n",
        "    llm = Replicate(\n",
        "        model=\"replicate/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1\",\n",
        "        input={\"temperature\": 0.01, \"max_new_tokens\": 500, \"repetition_penalty\": 1.15},\n",
        "    )\n",
        "\n",
        "    return llm\n"
      ],
      "metadata": {
        "id": "-9AT3CZ6DTzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = \"gpt-3.5-turbo-16k\"\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "# model_name = \"meta-llama/Llama-2-70b-chat-hf\"\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"OPENAI_API_BASE\"] = OPENAI_API_BASE\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n",
        "\n",
        "llm = get_llm(model_name)\n",
        "qa = get_qa_retriever(llm)\n",
        "\n",
        "# Here's where you would process the question and generate an answer.\n",
        "# For this simple example, the program just echoes the question back.\n",
        "answer = qa.run(query=\"what is gradient descent?\")\n",
        "\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cTwnT9WD2sY",
        "outputId": "9c9c9188-02db-4c4e-f4bd-71bfe0a5a166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a helpful data science tutor chatbot having a conversation with a human.\n",
            "                Follow exactly these 3 steps:\n",
            "                1. Read the Context below combining this information with the Question\n",
            "                2. Answer the Question using only the information in the Context section\n",
            "                3. Show the source for your answers\n",
            "\n",
            "                Context : gradient can be obtained by differentiating E from Equation (4.2), as \n",
            "where \n",
            "xid \n",
            "denotes the single input component \n",
            "xi \n",
            "for training example d. We now \n",
            "have an equation that gives in terms of the linear unit inputs \n",
            "xid, \n",
            "outputs \n",
            "Od, \n",
            "and target values \n",
            "td \n",
            "associated with the training examples. Substituting Equa- \n",
            "tion (4.6) into Equation (4.5) yields the weight update rule for gradient descent \n",
            "To summarize, the gradient descent algorithm for training linear units is as \n",
            "follows: Pick an initial random weight vector. Apply the linear unit to all training \n",
            "examples, then compute \n",
            "Awi \n",
            "for each weight according to Equation (4.7). Update \n",
            "each weight wi by adding Awi, then repeat this process. This algorithm is given \n",
            "in Table 4.1. Because the error surface contains only a single global minimum, \n",
            "this algorithm will converge to a weight vector with minimum error, regardless \n",
            "of whether the training examples are linearly separable, given a sufficiently small \n",
            "learning rate q is used. If r) is too large, the gradient descent search runs the risk \n",
            "of overstepping the minimum in the error surface rather than settling into it. For \n",
            "this reason, one common modification to the algorithm is to gradually reduce the \n",
            "value of r) as the number of gradient descent steps grows. \n",
            "4.4.3.3 STOCHASTIC APPROXIMATION TO GRADIENT DESCENT \n",
            "Gradient descent is an important general paradigm for learning. It is a strategy for \n",
            "searching through a large or infinite hypothesis space that can be applied whenever \n",
            "(1) the hypothesis space contains continuously parameterized hypotheses \n",
            "(e.g., \n",
            "the \n",
            "weights in a linear unit), and (2) the error can be differentiated with respect to \n",
            "these hypothesis parameters. The key practical difficulties in applying gradient \n",
            "descent are (1) converging to a local minimum can sometimes be quite slow (i.e., \n",
            "it can require many thousands of gradient descent steps), and (2) if there are \n",
            "multiple local minima in the error surface, then there is no guarantee that the \n",
            "procedure will find the global minimum.\n",
            "\n",
            "24\n",
            "Machine Learning for HumansGradient descent: learn the parameters\n",
            "“Put on a blindfold, take a step downhill. You’ve found the bottom when you have \n",
            "nowhere to go but up.”\n",
            "Gradient descent will come up over and over again, especially in neural networks. \n",
            "Machine learning libraries like scikit-learn and TensorFlow use it in the background everywhere, so it’s worth understanding the details.\n",
            "The goal of gradient descent is to find the minimum of our model’s loss function by \n",
            "iteratively getting a better and better approximation of it.\n",
            "Imagine yourself walking through a valley with a blindfold on. Your goal is to find the \n",
            "bottom of the valley. How would you do it?\n",
            "A reasonable approach would be to touch the ground around you and move in \n",
            "whichever direction the ground is sloping down most steeply. Take a step and repeat \n",
            "the same process continually until the ground is flat. Then you know you’ve reached the bottom of a valley; if you move in any direction from where you are, you’ll end up at the same elevation or further uphill.\n",
            "Going back to mathematics, the ground becomes our loss function, and the elevation \n",
            "at the bottom of the valley is the minimum of that function.\n",
            "Let’s take a look at the loss function we saw in regression:\n",
            "Part 2.1: Supervised Learning\n",
            "\n",
            "CHAPTER5.MACHINELEARNINGBASICS\n",
            "Gradientdescentingeneralhasoftenbeenregardedassloworunreliable.In\n",
            "thepast,theapplicationofgradientdescenttonon-convexoptimization problems\n",
            "wasregardedasfoolhardyorunprincipled. Today,weknowthatthemachine\n",
            "learningmodelsdescribedinpartworkverywellwhentrainedwithgradient II\n",
            "descent.Theoptimization algorithmmaynotbeguaranteedtoarriveatevena\n",
            "localminimuminareasonableamountoftime,butitoftenﬁndsaverylowvalue\n",
            "ofthecostfunctionquicklyenoughtobeuseful.\n",
            "Stochasticgradientdescenthasmanyimportantusesoutsidethecontextof\n",
            "deeplearning.Itisthemainwaytotrainlargelinearmodelsonverylarge\n",
            "datasets.Foraﬁxedmodelsize,thecostperSGDupdatedoesnotdependonthe\n",
            "trainingsetsizem.Inpractice,weoftenusealargermodelasthetrainingsetsize\n",
            "increases,butwearenotforcedtodoso.Thenumberofupdatesrequiredtoreach\n",
            "convergenceusuallyincreaseswithtrainingsetsize. However,asmapproaches\n",
            "inﬁnity,themodelwilleventuallyconvergetoitsbestpossibletesterrorbefore\n",
            "SGDhassampledeveryexampleinthetrainingset.Increasingmfurtherwillnot\n",
            "extendtheamountoftrainingtimeneededtoreachthemodel’sbestpossibletest\n",
            "error.Fromthispointofview,onecanarguethattheasymptoticcostoftraining\n",
            "amodelwithSGDisasafunctionof. O(1) m\n",
            "Priortotheadventofdeeplearning,themainwaytolearnnonlinearmodels\n",
            "wastousethekerneltrickincombinationwithalinearmodel.Manykernellearning\n",
            "algorithmsrequireconstructinganmm×matrixG i , j=k(x() i,x() j).Constructing\n",
            "thismatrixhascomputational costO(m2),whichisclearlyundesirablefordatasets\n",
            "with billions of examples. In academia, starting in2006,deep learning was\n",
            "initiallyinterestingbecauseitwasabletogeneralizetonewexamplesbetter\n",
            "thancompetingalgorithmswhentrainedonmedium-sizeddatasetswithtensof\n",
            "thousandsofexamples.Soonafter,deeplearninggarneredadditionalinterestin\n",
            "industry,becauseitprovidedascalablewayoftrainingnonlinearmodelsonlarge\n",
            "datasets.\n",
            "Stochasticgradientdescentandmanyenhancements toitaredescribedfurther\n",
            "inchapter.8\n",
            "5.10BuildingaMachineLearningAlgorithm\n",
            "Nearlyalldeeplearningalgorithmscanbedescribedasparticularinstancesof\n",
            "afairlysimplerecipe:combineaspeciﬁcationofadataset,acostfunction,an\n",
            "optimization procedureandamodel.\n",
            "Forexample,thelinearregressionalgorithmcombinesadatasetconsistingof\n",
            "1 5 3\n",
            "                User Question: what is gradient descent?\n",
            "\n",
            "                The answer and your response should only come from the Context section. If the question is\n",
            "                not related to the Context section, politely respond that you are tuned to only answer questions that\n",
            "                are related to data science. Use as much detail as possible when responding.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Gradient descent is a strategy for learning parameters in machine learning models. It involves iteratively updating the model's parameters to minimize the loss function by following the direction of the steepest descent of the error surface. The goal is to find the minimum of the loss function, which represents the best possible prediction performance. In other words, gradient descent is a method to optimize the model's parameters to fit the training data better, and it is widely used in various machine learning algorithms, including neural networks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ooxGheJ-eu2"
      },
      "source": [
        "Gradient descent is an iterative optimization algorithm used to find the minimum of a \\n                function. It works by taking small steps in the direction of the negative gradient of the function, \\n                with each step aiming to decrease the value of the function. The algorithm continues until the \\n                function's value no longer decreases or a stopping criterion is reached. Gradient descent is widely \\n                used in machine learning, particularly in neural networks, to find the optimal parameters that \\n                minimize the loss function.\\n\\n                Context:\\n24\\nMachine Learning for HumansGradient descent: learn the parameters\\n“Put on a blindfold, take a step downhill. You’ve found the bottom when you have \\nnowhere to go but up.”\\nGradient descent will come up over and over again, especially in neural networks. \\nMachine learning libraries like scikit-learn and TensorFlow use it in the background everywhere, so it’s worth understanding the details.\\nThe goal of gradient descent is to find the minimum of our model’s loss function by \\niteratively getting a better and better approximation of it.\\nImagine yourself walking through a valley with a blindfold on. Your goal is to find the \\nbottom of the valley. How would you do it?\\nA reasonable approach would be to touch the ground around you and move in \\nwhichever direction the ground is sloping down most steeply. Take a step and repeat \\nthe same process continually until the ground is flat. Then you know you’ve reached the bottom of a valley; if you move in any direction from where you are, you’ll end up at the same elevation or further uphill.\\nGoing back to mathematics, the ground becomes our loss function, and the elevation \\nat the b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zwY1aiEMxal",
        "outputId": "134e634f-efd4-4ab0-df37-286fa49851eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question_id': 133, 'text': 'What are the drawbacks of the linear model? ', 'category': 'knowledge'}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a helpful data science tutor chatbot having a conversation with a human.\n",
            "                Follow exactly these 3 steps:\n",
            "                1. Read the Context below combining this information with the Question\n",
            "                2. Answer the Question using only the information in the Context section\n",
            "                3. Show the source for your answers\n",
            "\n",
            "                Context : Part II\n",
            "Linear Models\n",
            "\n",
            "196 7. Linear models\n",
            "different from tree or rule models, where the structure of the model (e.g., which\n",
            "features to use in the tree, and where) is not ﬁxed in advance.\n",
            "/lefthandLinear models are stable, which is to say that small variations in the training data\n",
            "have only limited impact on the learned model. Tree models tend to vary more\n",
            "with the training data, as the choice of a different split at the root of the tree\n",
            "typically means that the rest of the tree is different as well.\n",
            "/lefthandLinear models are less likely to overﬁt the training data than some other models,\n",
            "largely because they have relatively few parameters. The ﬂipside of this is that\n",
            "they sometimes lead to underﬁtting : e.g., imagine you are learning where the\n",
            "border runs between two countries from labelled samples, then a linear model\n",
            "is unlikely to give a good approximation.\n",
            "The last two points can be summarised by saying that linear models have low variance\n",
            "but high bias. Such models are often preferable when you have limited data and want\n",
            "to avoid overﬁtting. High variance–low bias models such as decision trees are prefer-\n",
            "able if data is abundant but underﬁtting is a concern. It is usually a good idea to start\n",
            "with simple, high-bias models such as linear models and only move on to more elabo-\n",
            "rate models if the simpler ones appear to be underﬁtting.\n",
            "Linear models exist for all predictive tasks, including classiﬁcation, probability es-\n",
            "timation and regression. Linear regression, in particular, is a well-studied problem that\n",
            "can be solved by the least-squares method, which is the topic of the next section. We\n",
            "will look at a number of other linear models in this chapter, including least-squares\n",
            "classiﬁcation (also in Section 7.1 ), the perceptron in Section 7.2 , and the support vec-\n",
            "t o rm a c h i n ei n Section 7.3 . We will also ﬁnd out how these models can be turned into\n",
            "probability estimators in Section 7.4 . Finally, Section 7.5 brieﬂy discusses how each of\n",
            "these methods could learn non-linear models by means of so-called kernel functions.\n",
            "7.1 The least-squares method\n",
            "We start by introducing a method that can be used to learn linear models for classiﬁca-\n",
            "tion and regression. Recall that the regression problem is to learn a function estimator\n",
            "ˆf:X→Rfrom examples ( xi,f(xi)), where in this chapter we assume X=Rd. The dif-\n",
            "ferences between the actual and estimated function values on the training examples\n",
            "are called residuals /epsilon1i=f(xi)−ˆf(xi). The least-squares method , introduced by Carl\n",
            "Friedrich Gauss in the late eighteenth century, consists in ﬁnding ˆfsuch that/summationtextn\n",
            "i=1/epsilon12\n",
            "iis\n",
            "minimised. The following example illustrates the method in the simple case of a single\n",
            "feature, which is called univariate regression .\n",
            "\n",
            "7Moving Beyond Linearity\n",
            "So far in this book, we have mostly focused on linear models. Linear modelsare relatively simple to describe and implement, and have advantages overother approaches in terms of interpretation and inference. However, stan-dard linear regression can have signiﬁcant limitations in terms of predic-tive power. This is because the linearity assumption is almost always anapproximation, and sometimes a poor one. In Chapter 6 we see that we canimprove upon least squares using ridge regression, the lasso, principal com-ponents regression, and other techniques. In that setting, the improvementis obtained by reducing the complexity of the linear model, and hence thevariance of the estimates. But we are still using a linear model, which canonly be improved so far! In this chapter we relax the linearity assumptionwhile still attempting to maintain as much interpretability as possible. Wedo this by examining very simple extensions of linear models like polyno-mial regression and step functions, as well as more sophisticated approachessuch as splines, local regression, and generalized additive models.•Polynomial regressionextends the linear model by adding extra pre-dictors, obtained by raising each of the original predictors to a power.For example, acubicregression uses three variables,X,X2, andX3,as predictors. This approach provides a simple way to provide a non-linear ﬁt to data.•Step functionscut the range of a variable intoKdistinct regions inorder to produce a qualitative variable. This has the eﬀect of ﬁttinga piecewise constant function.© Springer Science+Business Media, LLC, part of Springer Nature 2021G. James et al., An Introduction to Statistical Learning, Springer Texts in Statistics,https://doi.org/10.1007/978-1-0716-1418-1_7289\n",
            "                User Question: What are the drawbacks of the linear model? \n",
            "\n",
            "                The answer and your response should only come from the Context section. If the question is\n",
            "                not related to the Context section, politely respond that you are tuned to only answer questions that\n",
            "                are related to data science. Use as much detail as possible when responding.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " The drawbacks of the linear model are:\n",
            "\n",
            "* High bias: Linear models tend to have high bias, which means that they sometimes lead to underfitting. This occurs when the model is too simple and cannot capture the underlying patterns in the data.\n",
            "* Limited flexibility: Linear models are relatively simple and inflexible compared to other machine learning models such as decision trees or neural networks. They can only capture linear relationships between the features and the target variable, and may not be able to capture more complex non-linear relationships.\n",
            "* Sensitivity to outliers: Linear models can be sensitive to outliers in the data, which can affect their performance. Outliers can skew the model's predictions and lead to poor generalization.\n",
            "* Limited interpretability: While linear models are generally easier to interpret than more complex models, they can still be difficult to understand, especially for non-experts. The coefficients of a linear model represent the change in the target variable for a one-unit change in a feature, but this may not be immediately intuitive for all users.\n",
            "\n",
            "Source:\n",
            "\n",
            "* \"Linear Models\" by G. James, D. C. Montgomery, and C. L. R. K. Reddy, in An Introduction to Statistical Learning, Springer Texts in Statistics, 2021, pp. 76-83.\n",
            "{'question_id': 134, 'text': 'How regularly must an algorithm be updated? ', 'category': 'knowledge'}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a helpful data science tutor chatbot having a conversation with a human.\n",
            "                Follow exactly these 3 steps:\n",
            "                1. Read the Context below combining this information with the Question\n",
            "                2. Answer the Question using only the information in the Context section\n",
            "                3. Show the source for your answers\n",
            "\n",
            "                Context : 14 CHAPTER 1. INTRODUCTION\n",
            "the size, the same algorithm should take no more than half as long to run. Furthermore,\n",
            "an ideal algorithm would maintain these desirable scaling characteristics across a wide\n",
            "range of settings: on data ranging from gigabytes to petabytes, on clusters consisting\n",
            "of a few to a few thousand machines. Finally, the ideal algorithm would exhibit these\n",
            "desired behaviors without requiring any modi\fcations whatsoever, not even tuning of\n",
            "parameters.\n",
            "Other than for embarrassingly parallel problems, algorithms with the character-\n",
            "istics sketched above are, of course, unobtainable. One of the fundamental assertions\n",
            "in Fred Brook's classic The Mythical Man-Month [28] is that adding programmers to a\n",
            "project behind schedule will only make it fall further behind. This is because complex\n",
            "tasks cannot be chopped into smaller pieces and allocated in a linear fashion, and is\n",
            "often illustrated with a cute quote: \\nine women cannot have a baby in one month\".\n",
            "Although Brook's observations are primarily about software engineers and the soft-\n",
            "ware development process, the same is also true of algorithms: increasing the degree\n",
            "of parallelization also increases communication costs. The algorithm designer is faced\n",
            "with diminishing returns, and beyond a certain point, greater e\u000eciencies gained by\n",
            "parallelization are entirely o\u000bset by increased communication requirements.\n",
            "Nevertheless, these fundamental limitations shouldn't prevent us from at least\n",
            "striving for the unobtainable. The truth is that most current algorithms are far from\n",
            "the ideal. In the domain of text processing, for example, most algorithms today assume\n",
            "that data \fts in memory on a single machine. For the most part, this is a fair assumption.\n",
            "But what happens when the amount of data doubles in the near future, and then doubles\n",
            "again shortly thereafter? Simply buying more memory is not a viable solution, as the\n",
            "amount of data is growing faster than the price of memory is falling. Furthermore, the\n",
            "price of a machine does not scale linearly with the amount of available memory beyond\n",
            "a certain point (once again, the scaling \\up\" vs. scaling \\out\" argument). Quite simply,\n",
            "algorithms that require holding intermediate data in memory on a single machine will\n",
            "simply break on su\u000eciently-large datasets|moving from a single machine to a cluster\n",
            "architecture requires fundamentally di\u000berent algorithms (and reimplementations).\n",
            "Perhaps the most exciting aspect of MapReduce is that it represents a small step\n",
            "toward algorithms that behave in the ideal manner discussed above. Recall that the\n",
            "programming model maintains a clear separation between what computations need to\n",
            "occur with howthose computations are actually orchestrated on a cluster. As a result,\n",
            "a MapReduce algorithm remains \fxed, and it is the responsibility of the execution\n",
            "framework to execute the algorithm. Amazingly, the MapReduce programming model\n",
            "is simple enough that it is actually possible, in many circumstances, to approach the\n",
            "ideal scaling characteristics discussed above. We introduce the idea of the \\tradeable\n",
            "machine hour\", as a play on Brook's classic title. If running an algorithm on a particular\n",
            "dataset takes 100 machine hours, then we should be able to \fnish in an hour on a cluster\n",
            "\n",
            "7.1. LIMITATIONS OF MAPREDUCE 153\n",
            "updates are not made until the full evidence of the training data has been weighed\n",
            "against the model. An earlier update would seem, in some sense, to be hasty. However,\n",
            "it is generally the case that more frequent updates can lead to more rapid convergence\n",
            "of the model (in terms of number of training instances processed), even if those updates\n",
            "are made by considering lessdata [24]. Thinking in terms of gradient optimization (see\n",
            "Section 6.5), online learning algorithms can be understood as computing an approx-\n",
            "imation of the true gradient, using only a few training instances. Although only an\n",
            "approximation, the gradient computed from a small subset of training instances is of-\n",
            "ten quite reasonable, and the aggregate behavior of multiple updates tends to even out\n",
            "errors that are made. In the limit, updates can be made after every training instance.\n",
            "Unfortunately, implementing online learning algorithms in MapReduce is problem-\n",
            "atic. The model parameters in a learning algorithm can be viewed as shared global state,\n",
            "which must be updated as the model is evaluated against training data. All processes\n",
            "performing the evaluation (presumably the mappers) must have access to this state.\n",
            "In a batch learner, where updates occur in one or more reducers (or, alternatively, in\n",
            "the driver code), synchronization of this resource is enforced by the MapReduce frame-\n",
            "work. However, with online learning, these updates must occur after processing smaller\n",
            "numbers of instances. This means that the framework must be altered to support faster\n",
            "processing of smaller datasets, which goes against the design choices of most existing\n",
            "MapReduce implementations. Since MapReduce was speci\fcally optimized for batch\n",
            "operations over large amounts of data, such a style of computation would likely result\n",
            "in ine\u000ecient use of resources. In Hadoop, for example, map and reduce tasks have con-\n",
            "siderable startup costs. This is acceptable because in most circumstances, this cost is\n",
            "amortized over the processing of many key-value pairs. However, for small datasets,\n",
            "these high startup costs become intolerable. An alternative is to abandon shared global\n",
            "state and run independent instances of the training algorithm in parallel (on di\u000berent\n",
            "portions of the data). A \fnal solution is then arrived at by merging individual results.\n",
            "Experiments, however, show that the merged solution is inferior to the output of running\n",
            "the training algorithm on the entire dataset [52].\n",
            "A related di\u000eculty occurs when running what are called Monte Carlo simula-\n",
            "tions , which are used to perform inference in probabilistic models where evaluating or\n",
            "representing the model exactly is impossible. The basic idea is quite simple: samples\n",
            "are drawn from the random variables in the model to simulate its behavior, and then\n",
            "simple frequency statistics are computed over the samples. This sort of inference is par-\n",
            "ticularly useful when dealing with so-called nonparametric models , which are models\n",
            "whose structure is not speci\fed in advance, but is rather inferred from training data.\n",
            "For an illustration, imagine learning a hidden Markov model, but inferring the num-\n",
            "ber of states, rather than having them speci\fed. Being able to parallelize Monte Carlo\n",
            "simulations would be tremendously valuable, particularly for unsupervised learning ap-\n",
            "\n",
            "Numerical\n",
            "Algorithms\n",
            "                User Question: How regularly must an algorithm be updated? \n",
            "\n",
            "                The answer and your response should only come from the Context section. If the question is\n",
            "                not related to the Context section, politely respond that you are tuned to only answer questions that\n",
            "                are related to data science. Use as much detail as possible when responding.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Based on the provided context, there is no specific mention of how frequently an algorithm should be updated. However, it does discuss the challenges of implementing online learning algorithms in MapReduce, which may require more frequent updates to the model parameters. It also mentions that updates can be made after every training instance, but this may lead to inefficient use of resources. Therefore, it's implied that the updating frequency depends on the specific use case and available resources. In general, it's important to strike a balance between update frequency and computational efficiency.\n",
            "{'question_id': 135, 'text': 'What are eigenvalues and eigenvectors? ', 'category': 'knowledge'}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a helpful data science tutor chatbot having a conversation with a human.\n",
            "                Follow exactly these 3 steps:\n",
            "                1. Read the Context below combining this information with the Question\n",
            "                2. Answer the Question using only the information in the Context section\n",
            "                3. Show the source for your answers\n",
            "\n",
            "                Context : 4.2 Eigenvalues and Eigenvectors 113\n",
            "Figure 4.1\n",
            "Geometric\n",
            "interpretation of\n",
            "eigenvalues. The\n",
            "eigenvectors of A\n",
            "get stretched by the\n",
            "corresponding\n",
            "eigenvalues. The\n",
            "area of the unit\n",
            "square changes by\n",
            "|λ1λ2|, the\n",
            "perimeter changes\n",
            "by a factor of\n",
            "1\n",
            "2(|λ1|+|λ2|).x1x2\n",
            "v1v2A\n",
            "Theorem 4.17. The trace of a matrix A∈Rn×nis the sum of its eigenval-\n",
            "ues, i.e.,\n",
            "tr(A) =nX\n",
            "i=1λi, (4.43)\n",
            "where λi∈Care (possibly repeated) eigenvalues of A.\n",
            "Let us provide a geometric intuition of these two theorems. Consider\n",
            "a matrix A∈R2×2that possesses two linearly independent eigenvectors\n",
            "x1,x2. For this example, we assume (x1,x2)are an ONB of R2so that they\n",
            "are orthogonal and the area of the square they span is 1; see Figure 4.1.\n",
            "From Section 4.1, we know that the determinant computes the change of\n",
            "area of unit square under the transformation A. In this example, we can\n",
            "compute the change of area explicitly: Mapping the eigenvectors using\n",
            "Agives us vectors v1=Ax1=λ1x1andv2=Ax2=λ2x2, i.e., the\n",
            "new vectors viare scaled versions of the eigenvectors xi, and the scaling\n",
            "factors are the corresponding eigenvalues λi.v1,v2are still orthogonal,\n",
            "and the area of the rectangle they span is |λ1λ2|.\n",
            "Given that x1,x2(in our example) are orthonormal, we can directly\n",
            "compute the perimeter of the unit square as 2(1 + 1) . Mapping the eigen-\n",
            "vectors using Acreates a rectangle whose perimeter is 2(|λ1|+|λ2|).\n",
            "Therefore, the sum of the absolute values of the eigenvalues tells us how\n",
            "the perimeter of the unit square changes under the transformation matrix\n",
            "A.\n",
            "Example 4.9 (Google’s PageRank – Webpages as Eigenvectors)\n",
            "Google uses the eigenvector corresponding to the maximal eigenvalue of\n",
            "a matrix Ato determine the rank of a page for search. The idea for the\n",
            "PageRank algorithm, developed at Stanford University by Larry Page and\n",
            "Sergey Brin in 1996, was that the importance of any web page can be ap-\n",
            "proximated by the importance of pages that link to it. For this, they write\n",
            "down all web sites as a huge directed graph that shows which page links\n",
            "to which. PageRank computes the weight (importance) xi⩾0of a web\n",
            "siteaiby counting the number of pages pointing to ai. Moreover, PageR-\n",
            "ank takes into account the importance of the web sites that link to ai. The\n",
            "navigation behavior of a user is then modeled by a transition matrix Aof\n",
            "this graph that tells us with what (click) probability somebody will end up\n",
            "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
            "\n",
            "CHAPTER2.LINEARALGEBRA\n",
            "\u0000 \u0000 \u0000    \n",
            "\u0000\u0000\u0000 \n",
            "                     \n",
            "\u0000 \u0000 \u0000    \n",
            "\n",
            "\u0000\u0000\u0000\n",
            "   \n",
            "                                                     \n",
            "Figure2.3:Anexampleoftheeﬀectofeigenvectorsandeigenvalues.Here,wehave\n",
            "amatrixAwithtwoorthonormaleigenvectors,v( 1 )witheigenvalue λ 1andv( 2 )with\n",
            "eigenvalue λ 2. ( L e f t )Weplotthesetofallunitvectorsu∈ R2asaunitcircle. ( R i g h t )We\n",
            "plotthesetofallpointsAu.ByobservingthewaythatAdistortstheunitcircle,we\n",
            "canseethatitscalesspaceindirectionv( ) iby λ i.\n",
            "eigenvectorstoformamatrixVwithoneeigenvectorpercolumn:V= [v( 1 ), . . . ,\n",
            "v( ) n].Likewise,wecanconcatenatetheeigenvaluestoformavectorλ= [ λ 1 , . . . ,\n",
            "λ n].The ofisthengivenby eigendecompositionA\n",
            "AVλV = diag()− 1. (2.40)\n",
            "Wehaveseenthatconstructingmatriceswithspeciﬁceigenvaluesandeigenvec-\n",
            "torsallowsustostretchspaceindesireddirections. Ho wever,weoftenwantto\n",
            "decomposematricesintotheireigenvaluesandeigenvectors.Doingsocanhelp\n",
            "ustoanalyzecertainpropertiesofthematrix,muchasdecomposinganinteger\n",
            "intoitsprimefactorscanhelpusunderstandthebehaviorofthatinteger.\n",
            "Noteverymatrixcanbedecomposedintoeigenvaluesandeigenvectors.Insome\n",
            "4 3\n",
            "\n",
            "Linear Algebra\n",
            "De\fnition 126 (Eigenvalues and Eigenvectors) .For a square matrix A,eis an eigenvector of Awith\n",
            "eigenvalue\u0015if\n",
            "Ae=\u0015e (A.1.39)\n",
            "det (A) =nY\n",
            "i=1\u0015i (A.1.40)\n",
            "Hence a matrix is singular if it has a zero eigenvalue. The trace of a matrix can be expressed as\n",
            "trace ( A) =X\n",
            "i\u0015i (A.1.41)\n",
            "For an (n\u0002n) dimensional matrix, there are (including repetitions) neigenvalues, each with a correspond-\n",
            "ing eigenvector. We can reform equation (A.1.39) as\n",
            "(A\u0000\u0015I)e=0 (A.1.42)\n",
            "This is a linear equation, for which the eigenvector eand eigenvalue \u0015is a solution. We can write equation\n",
            "(A.1.42) as Be=0, where B\u0011A\u0000\u0015I. IfBhas an inverse, then a solution is e=B\u000010=0, which\n",
            "trivially satis\fes the eigen-equation. For any non-trivial solution to the problem Be=0, we therefore\n",
            "needBto be non-invertible. This is equivalent to the condition that Bhas zero determinant. Hence \u0015is\n",
            "an eigenvalue of Aif\n",
            "det (A\u0000\u0015I) = 0 (A.1.43)\n",
            "This is known as the characteristic equation. This determinant equation will be a polynomial of degree n\n",
            "and the resulting equation is known as the characteristic polynomial. Once we have found an eigenvalue,\n",
            "the corresponding eigenvector can be found by substituting this value for \u0015in equation (A.1.39) and solving\n",
            "the linear equations for e. It may be that the for an eigenvalue \u0015the eigenvector is not unique and there\n",
            "is a space of corresponding vectors. Geometrically, the eigenvectors are special directions such that the\n",
            "e\u000bect of the transformation Aalong a direction eis simply to scale the vector e. For a rotation matric R\n",
            "in general there will be no direction preserved under the rotation so that the eigenvalues and eigenvectors\n",
            "are complex valued (which is why the Fourier representation, which corresponds to representation in a\n",
            "rotated basis, is necessarily complex).\n",
            "Remark 11 (Orthogonality of eigenvectors of symmetric matrices) .For a real symmetric matric\n",
            "A=AT, and two of its eigenvectors eiandejofAare orthogonal ( ei)Tej= 0 if the eigenvalues \u0015i\n",
            "and\u0015jare di\u000berent.\n",
            "The above can be shown by considering:\n",
            "Aei=\u0015iei)(ej)TAei=\u0015i(ej)Tei(A.1.44)\n",
            "Since Ais symmetric, the left hand side is equivalent to\n",
            "((ej)TA)ei= (Aej)Tei=\u0015j(ej)Tei)\u0015i(ej)Tei=\u0015j(ej)Tei(A.1.45)\n",
            "If\u0015i6=\u0015j, this condition can be satis\fed only if ( ej)Tei= 0, namely that the eigenvectors are orthogonal.\n",
            "DRAFT March 9, 2010 549\n",
            "                User Question: What are eigenvalues and eigenvectors? \n",
            "\n",
            "                The answer and your response should only come from the Context section. If the question is\n",
            "                not related to the Context section, politely respond that you are tuned to only answer questions that\n",
            "                are related to data science. Use as much detail as possible when responding.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Eigenvalues and eigenvectors are concepts from linear algebra that are used to understand the behavior of matrices. An eigenvalue is a scalar that represents how much a matrix stretches or compresses a vector, while an eigenvector is a non-zero vector that does not change direction when a matrix is multiplied by it. In other words, eigenvectors are special directions that do not change under the transformation defined by a matrix.\n",
            "\n",
            "Given a square matrix A, an eigenvector e of A with eigenvalue λ satisfies the equation:\n",
            "\n",
            "Ae = λe\n",
            "\n",
            "This means that when we multiply A by e, we get back a scaled version of e, where the scaling factor is λ. The eigenvectors of a matrix A can be thought of as the \"principal components\" of the space that A acts on. They are special because they are the directions in which the matrix stretches or compresses the most.\n",
            "\n",
            "Eigenvalues and eigenvectors play a crucial role in many applications in data science, including dimensionality reduction, feature selection, and clustering. For example, principal component analysis (PCA) is a technique that is used to reduce the dimensionality of high-dimensional datasets by finding the eigenvectors of the covariance matrix of the data. These eigenvectors represent the directions of maximum variance in the data, and can be used to project the data onto a lower-dimensional space while preserving most of the information in the original dataset.\n",
            "{'question_id': 136, 'text': 'Why is resampling done? ', 'category': 'knowledge'}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a helpful data science tutor chatbot having a conversation with a human.\n",
            "                Follow exactly these 3 steps:\n",
            "                1. Read the Context below combining this information with the Question\n",
            "                2. Answer the Question using only the information in the Context section\n",
            "                3. Show the source for your answers\n",
            "\n",
            "                Context : 5Resampling Methods\n",
            "Resampling methodsare an indispensable tool in modern statistics. Theyinvolve repeatedly drawing samples from a training set and reﬁtting a modelof interest on each sample in order to obtain additional information aboutthe ﬁtted model. For example, in order to estimate the variability of a linearregression ﬁt, we can repeatedly draw diﬀerent samples from the trainingdata, ﬁt a linear regression to each new sample, and then examine theextent to which the resulting ﬁts diﬀer. Such an approach may allow us toobtain information that would not be available from ﬁtting the model onlyonce using the original training sample.Resampling approaches can be computationally expensive, because theyinvolve ﬁtting the same statistical method multiple times using diﬀerentsubsets of the training data. However, due to recent advances in computingpower, the computational requirements of resampling methods generallyare not prohibitive. In this chapter, we discuss two of the most commonlyused resampling methods,cross-validationand thebootstrap. Both methodsare important tools in the practical application of many statistical learningprocedures. For example, cross-validation can be used to estimate the testerror associated with a given statistical learning method in order to evaluateits performance, or to select the appropriate level of ﬂexibility. The processof evaluating a model’s performance is known asmodel assessment, whereasmodelassessmentthe process of selecting the proper level of ﬂexibility for a model is known asmodel selection. The bootstrap is used in several contexts, most commonlymodelselectionto provide a measure of accuracy of a parameter estimate or of a givenstatistical learning method.© Springer Science+Business Media, LLC, part of Springer Nature 2021G. James et al., An Introduction to Statistical Learning, Springer Texts in Statistics,197https://doi.org/10.1007/978-1-0716-1418-1_5\n",
            "\n",
            "9.5. RESAMPLING FOR CLASSIFIER DESIGN 25\n",
            "duplication of individual points in a bootstrap data set.) In bootstrap estimation,∗\n",
            "this selection process is independently repeated Btimes to yield Bbootstrap data\n",
            "sets, which are treated as independent sets. The bootstrap estimate of a statistic θ,\n",
            "denoted ˆθ∗(·), is merely the mean of the Bestimates on the individual bootstrap data\n",
            "sets:\n",
            "ˆθ∗(·)=1\n",
            "BB/summationdisplay\n",
            "b=1ˆθ∗(b), (33)\n",
            "where ˆθ∗(b)is the estimate on bootstrap sample b.\n",
            "Bootstrap bias estimate\n",
            "The bootstrap estimate of the bias is (Problem ??)\n",
            "biasboot=1\n",
            "BB/summationdisplay\n",
            "b=1ˆθ∗(b)−ˆθ=ˆθ∗(·)−ˆθ. (34)\n",
            "Computer exercise 45 shows how the bootstrap can be applied to statistics that resist\n",
            "computational analysis, such as the “trimmed mean,” in which the mean is calculated trimmed\n",
            "mean for a distribution in which some percentage (e.g., 5%) of the high and the low points\n",
            "in a distribution have been eliminated.\n",
            "Bootstrap variance estimate\n",
            "The bootstrap estimate of the variance is\n",
            "Varboot[θ]=1\n",
            "BB/summationdisplay\n",
            "b=1/bracketleftBig\n",
            "ˆθ∗(b)−ˆθ∗(·)/bracketrightBig2\n",
            ". (35)\n",
            "If the statistic θis the mean, then in the limit of B→∞, the bootstrap estimate of\n",
            "the variance is the traditional variance of the mean (Problem 22). Generally speaking,the larger the number Bof bootstrap samples, the more satisfactory is the estimate\n",
            "of a statistic and its variance. One of the beneﬁts of bootstrap estimation is that B\n",
            "can be adjusted to the computational resources; if powerful computers are availablefor a long time, then Bcan be chosen large. In contrast, a jackknife estimate requires\n",
            "exactly nrepetitions: fewer repetitions gives a poorer estimate that depends upon\n",
            "the random points chosen; more repetitions merely duplicates information alreadyprovided by some of the ﬁrst nleave-one-out calculations.\n",
            "9.5 Resampling for classiﬁer design\n",
            "The previous section addressed the use of resampling in estimating statistics, including\n",
            "the accuracy of an existing classiﬁer, but only indirectly referred to the design ofclassiﬁers themselves. We now turn to a number of general resampling methods thathave proven eﬀective when used in conjunction with any in a wide range of techniques\n",
            "∗“Bootstrap” comes from Rudolf Erich Raspe’s wonderful stories “The adventures of Baron Munch-\n",
            "hausen,” in which the hero could pull himself up onto his horse by lifting his own bootstraps. Adiﬀerent but more common usage of the term applies to starting a computer, which must ﬁrst runa program before it can run other programs.\n",
            "\n",
            "212 5. Resampling Methods\n",
            "2.8 5.3 3 1.1 2.1 2 2.4 4.3 1 Y X Obs 2.8 5.3 3 2.4 4.3 1 2.8 5.3 3 Y X Obs \n",
            "2.4 4.3 1 2.8 5.3 3 1.1 2.1 2 Y X Obs \n",
            "2.4 4.3 1 1.1 2.1 2 1.1 2.1 2 Y X Obs Original Data (Z) 1*Z2*ZZ*B1*ˆα\n",
            "2*ˆαˆα*B/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1FIGURE 5.11.A graphical illustration of the bootstrap approach on a smallsample containingn=3observations. Each bootstrap data set containsnobser-vations, sampled with replacement from the original data set. Each bootstrap dataset is used to obtain an estimate ofα.puted using a distinct bootstrap data set. This panel was constructed onthe basis of a single data set, and hence could be created using real data.Note that the histogram looks very similar to the left-hand panel, whichdisplays the idealized histogram of the estimates ofαobtained by gener-ating 1,000 simulated data sets from the true population. In particular thebootstrap estimate SE(ˆα) from (5.8) is 0.087, very close to the estimateof 0.083 obtained using 1,000 simulated data sets. The right-hand paneldisplays the information in the center and left panels in a diﬀerent way, viaboxplots of the estimates forαobtained by generating 1,000 simulated datasets from the true population and using the bootstrap approach. Again, theboxplots have similar spreads, indicating that the bootstrap approach canbe used to eﬀectively estimate the variability associated with ˆα.5.3 Lab: Cross-Validation and the BootstrapIn this lab, we explore the resampling techniques covered in this chapter.Some of the commands in this lab may take a while to run on your com-puter.\n",
            "                User Question: Why is resampling done? \n",
            "\n",
            "                The answer and your response should only come from the Context section. If the question is\n",
            "                not related to the Context section, politely respond that you are tuned to only answer questions that\n",
            "                are related to data science. Use as much detail as possible when responding.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Resampling is done to obtain additional information about a fitted model. It involves repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to examine the extent to which the resulting fits differ. This allows us to obtain information that would not be available from fitting the model only once using the original training sample. Resampling approaches can be computationally expensive, but they are useful tools in the practical application of many statistical learning procedures, such as model assessment and model selection.\n",
            "{'question_id': 137, 'text': 'What are the important skills to have in Python with regard to data analysis? ', 'category': 'knowledge'}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a helpful data science tutor chatbot having a conversation with a human.\n",
            "                Follow exactly these 3 steps:\n",
            "                1. Read the Context below combining this information with the Question\n",
            "                2. Answer the Question using only the information in the Context section\n",
            "                3. Show the source for your answers\n",
            "\n",
            "                Context : CHAPTER1\n",
            "IMPORTING , SUMMARIZING ,AND\n",
            "VISUALIZING DATA\n",
            "This chapter describes where to ﬁnd useful data sets, how to load them into Python,\n",
            "and how to (re)structure the data. We also discuss various ways in which the data can\n",
            "be summarized via tables and ﬁgures. Which type of plots and numerical summaries\n",
            "are appropriate depends on the type of the variable(s) in play. Readers unfamiliar with\n",
            "Python are advised to read Appendix D ﬁrst.\n",
            "1.1 Introduction\n",
            "Data comes in many shapes and forms, but can generally be thought of as being the result\n",
            "of some random experiment — an experiment whose outcome cannot be determined in\n",
            "advance, but whose workings are still subject to analysis. Data from a random experiment\n",
            "are often stored in a table or spreadsheet. A statistical convention is to denote variables —\n",
            "often called features features — as columns and the individual items (or units) as rows . It is useful\n",
            "to think of three types of columns in such a spreadsheet:\n",
            "1. The ﬁrst column is usually an identiﬁer or index column, where each unit /row is\n",
            "given a unique name or ID.\n",
            "2. Certain columns (features) can correspond to the design of the experiment, specify-\n",
            "ing, for example, to which experimental group the unit belongs. Often the entries in\n",
            "these columns are deterministic ; that is, they stay the same if the experiment were to\n",
            "be repeated.\n",
            "3. Other columns represent the observed measurements of the experiment. Usually,\n",
            "these measurements exhibit variability ; that is, they would change if the experiment\n",
            "were to be repeated.\n",
            "There are many data sets available from the Internet and in software packages. A well-\n",
            "known repository of data sets is the Machine Learning Repository maintained by the Uni-\n",
            "versity of California at Irvine (UCI), found at https://archive.ics.uci.edu/ .\n",
            "1\n",
            "\n",
            "Giving Computers the Ability  to Learn from Data[ 14 ]We can think of those hyperparameters as parameters that are not learned from the \n",
            "data but represent the knobs of a model that we can turn to improve its performance. This will become much clearer in later chapters when we see actual\n",
            " examples.\n",
            "Evaluating models and predicting unseen data instances\n",
            "After we have selected a model that has been fitted on the training dataset, we can use the test dataset to estimate how well it performs on this unseen data to estimate the so-called generalization error. If we are satisfied with its performance, we can now \n",
            "use this model to predict new, future data. It is important to note that the parameters \n",
            "for the previously mentioned procedures, such as feature scaling and dimensionality reduction, are solely obtained from the training dataset, and the same\n",
            " parameters are \n",
            "later reapplied to transform the test dataset, as well as any new  data instances—the \n",
            "performance measured on the test data may be overly optimistic otherwise.\n",
            "Using Python for machine learning\n",
            "Python is one of the most popular programming languages for data science and \n",
            "thanks to its very active developer and open source community, a large number of useful libraries for scientific computing and machine learning have been developed.\n",
            "Although the performance of interpreted languages, such as Python, for \n",
            "computation-intensive tasks is inferior to lower-level programming languages, extension libraries such as NumPy and SciPy have been developed that build \n",
            "upon lower-layer Fortran and C implementations for fast vectorized operations on\n",
            " multidimensional arrays.\n",
            "For machine learning programming tasks, we will mostly refer to the scikit-learn \n",
            "library, which is currently one of the most popular and accessible open source \n",
            "machine learning libraries. In the later chapters, when we focus on a subfield of machine learning called deep learning, we will use the latest version of the TensorFlow library, which specializes in training so-called deep neural network \n",
            "models very efficiently by utilizing graphics cards.\n",
            "Installing Python and packages from the \n",
            "Python Package Index\n",
            "Python is available for all three major operating systems—Microsoft Windows, macOS, and Linux—and the installer, as well as the documentation, can be downloaded from the official Python  website: \n",
            "https://www.python.org .\n",
            "\n",
            "Giving Computers the Ability  to Learn from Data[ 16 ]Packages for scientific computing, data \n",
            "science, and machine learning\n",
            "Throughout this book, we will mainly use NumPy's multidimensional arrays to store and manipulate data. Occasionally, we will make use of pandas, which is a library built on top of NumPy that provides additional higher-level data manipulation \n",
            "tools that make working with tabular data even more convenient. To augment your \n",
            "learning experience and visualize quantitative data, which is often extremely useful to make sense of it, we will use the very customizable Matplotlib library.\n",
            "The version numbers of the major Python packages that were used to write this \n",
            "book are mentioned in the following list. Please make sure that the version numbers of your installed packages are equal to, or greater than, these version numbers to ensure that the code examples run correctly:\n",
            "•\n",
            " NumPy 1.17.4\n",
            "• SciPy 1.3.1\n",
            "• scikit-learn 0.22.0\n",
            "• Matplotlib 3.1.0\n",
            "• pandas 0.25.3\n",
            "Summary\n",
            "In this chapter, we explored machine learning at a very high level and familiarized \n",
            "ourselves with the big picture and major concepts that we are going to explore in the following chapters in more detail. We learned that supervised learning is composed \n",
            "of two important subfields: classification and regression. While classification models \n",
            "allow us to categorize objects into known classes, we can use regression analysis to predict the continuous outcomes of target variables. Unsupervised learning not only offers useful techniques for discovering structures in unlabeled data, but it can also be useful for data compression in feature preprocessing steps.\n",
            "We briefly went over the typical roadmap for applying machine learning to \n",
            "problem \n",
            " tasks, which we will use as a foundation for deeper discussions and hands-\n",
            "on examples in the following chapters. Finally, we set up our Python environment \n",
            "and installed and updated the required packages to get ready to see machine \n",
            "learning in action.\n",
            "                User Question: What are the important skills to have in Python with regard to data analysis? \n",
            "\n",
            "                The answer and your response should only come from the Context section. If the question is\n",
            "                not related to the Context section, politely respond that you are tuned to only answer questions that\n",
            "                are related to data science. Use as much detail as possible when responding.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Based on the context provided, it appears that the user is asking about the important skills to have in Python for data analysis. Here's an excerpt from the context that relates to this question:\n",
            "\n",
            "\"...Thanks to its very active developer and open source community, a large number of useful libraries for scientific computing and machine learning have been developed....\"\n",
            "\n",
            "In summary, having skills in Python programming is essential for data analysis, particularly in the field of machine learning. Familiarity with popular libraries such as NumPy, SciPy, scikit-learn, Matplotlib, and pandas is crucial for working with multidimensional arrays, manipulating data, and visualizing quantitative data. Additionally, knowledge of supervised and unsupervised learning techniques, as well as the ability to apply these techniques to real-world problems, is valuable.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import shortuuid\n",
        "\n",
        "answer_file = '/content/answer_llama-2-70b-chat.jsonl'\n",
        "#answer_file = '/content/answer_gpt-3.5-turbo.jsonl'\n",
        "\n",
        "model_id = 'llama-2-70b-chat'\n",
        "# model_id = 'gpt-3.5-turbo'\n",
        "\n",
        "# Open the input file for reading and the output file for writing\n",
        "with open('/content/question.jsonl', 'r') as infile, open(answer_file, 'w') as outfile:\n",
        "    for line in infile:\n",
        "        # Parse the JSON line into a Python dictionary\n",
        "        question = json.loads(line)\n",
        "        print(question)\n",
        "\n",
        "        answer = qa.run(query=question['text'])\n",
        "        print(answer)\n",
        "\n",
        "        # Create a dictionary with the answer and other required fields\n",
        "        answer_data = {\n",
        "            'answer_id': shortuuid.uuid(),\n",
        "            'model_id': model_id,\n",
        "            'question_id': question['question_id'],\n",
        "            'text': answer,\n",
        "            'metadata': {}\n",
        "        }\n",
        "\n",
        "        # Convert the Python dictionary back to a JSON string and write it to the output file\n",
        "        outfile.write(json.dumps(answer_data) + '\\n')  # add a newline character to make it a valid JSONL file\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}